{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import sys\n",
    "import inspect\n",
    "from urllib.parse import urljoin, urlsplit\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import aiohttp\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Page:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.status = None\n",
    "        self.text = ''\n",
    "        self.content = b''\n",
    "        self._soup = None\n",
    "        \n",
    "    @property\n",
    "    def soup(self):\n",
    "        if self._soup is None:\n",
    "            self._soup = BeautifulSoup(self.text, 'html.parser')\n",
    "            \n",
    "        return self._soup\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '<{}: url={}>'.format(\n",
    "                self.__class__.__name__,\n",
    "                self.url if len(self.url) < 37 else self.url[:37] + '...')\n",
    "        \n",
    "class Job:\n",
    "    def __init__(self, name, url, method='GET', headers=None, cookies=None):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.method = method\n",
    "        self.headers = headers\n",
    "        self.cookies = cookies\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '<{}: name=\"{}\" url={}>'.format(\n",
    "                self.__class__.__name__,\n",
    "                self.name,\n",
    "                self.url if len(self.url) < 27 else self.url[:27] + '...')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaseCrawler:\n",
    "    initial_urls = ['http://google.com/']\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or {}\n",
    "        self.loop = asyncio.get_event_loop()\n",
    "        self.tasks = []\n",
    "        \n",
    "        # Use a semaphore for rate limiting so we only have a set number of\n",
    "        # concurrent connections\n",
    "        self.lock = asyncio.Semaphore(self.config.get('max-connections', 10),\n",
    "                                      loop=self.loop)\n",
    "        \n",
    "        # Set up logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "        \n",
    "        log_file = self.config.get('log-file', 'stderr')\n",
    "\n",
    "        if log_file == 'stdout':\n",
    "            handler = logging.StreamHandler(sys.stdout)\n",
    "        elif log_file == 'stderr':\n",
    "            handler = logging.StreamHandler(sys.stderr)\n",
    "        else:\n",
    "            handler = logging.FileHandler(log_file)\n",
    "\n",
    "        self.logger.handlers.clear()\n",
    "        \n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s: %(message)s',\n",
    "            datefmt='%Y/%m/%d %I:%M:%S %p'\n",
    "        )\n",
    "        handler.setFormatter(formatter)\n",
    "        self.logger.addHandler(handler)\n",
    "        \n",
    "    def prepare(self):\n",
    "        self.base_url = 'http://google.com/'\n",
    "    \n",
    "    async def get_page(self, url, method='GET', headers=None, cookies=None):\n",
    "        with (await self.lock):\n",
    "            async with aiohttp.ClientSession(headers=headers, cookies=cookies) as session:\n",
    "                async with session.request(method, url) as resp:\n",
    "                    page = Page(url)\n",
    "                    page.status = resp.status\n",
    "                    page.text = await resp.text()\n",
    "                    page.content = await resp.read()\n",
    "                    return page\n",
    "                \n",
    "    async def handle(self, job):     \n",
    "        self.logger.debug('Fetching %s', job.url)\n",
    "        page = await self.get_page(\n",
    "                job.url,\n",
    "                job.method,\n",
    "                job.headers,\n",
    "                job.cookies)\n",
    "        self.logger.debug('Page \"%s\" is %d bytes', job.url, len(page.content))\n",
    "        \n",
    "        # Get the job handler\n",
    "        handler_name = 'handle_{}'.format(job.name)\n",
    "        handler = getattr(self, handler_name, self.default_handler)\n",
    "                    \n",
    "        self.logger.debug('Using %s() to handle job: %r', handler.__name__, job)\n",
    "        \n",
    "        # Do something with the data received\n",
    "        if inspect.isgeneratorfunction(handler):\n",
    "            for task in handler(page, job):\n",
    "                self.queue(task)\n",
    "        else:\n",
    "            result = handler(page, job)\n",
    "            \n",
    "            if result is not None:\n",
    "                self.queue(result)\n",
    "            \n",
    "    def handle_initial(self, page, job):\n",
    "        for link in page.soup.find_all('a'):\n",
    "            href = link['href']\n",
    "            \n",
    "            if href.startswith('/'):\n",
    "                href = urljoin(self.base_url, href)\n",
    "\n",
    "            new_job = Job('blah', href)\n",
    "            yield new_job\n",
    "    \n",
    "    def default_handler(self, page, job):\n",
    "        raise NotImplementedError('Handler not implemented for job: {}'.format(job))\n",
    "        \n",
    "    def queue(self, job):\n",
    "        task = asyncio.ensure_future(self.handle(job), loop=self.loop)\n",
    "        self.tasks.append(task)\n",
    "        self.logger.debug('Queued a job, %r', job)\n",
    "        return task\n",
    "\n",
    "    def run(self):\n",
    "        self.prepare()\n",
    "        \n",
    "        for url in self.initial_urls:\n",
    "            some_task = Job('initial', url)\n",
    "            self.queue(some_task)\n",
    "        \n",
    "        # While all tasks aren't either cancelled or done, run the loop until\n",
    "        # All currently queued tasks are done. Then check again.\n",
    "        while not all(t.cancelled() or t.done() for t in asyncio.Task.all_tasks(loop=self.loop)):\n",
    "            self.loop.run_until_complete(asyncio.gather(*self.tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bot = BaseCrawler()\n",
    "bot.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/michael/Documents/async_crawler'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
